{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding scripts to the path of the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\KifiyaAIM-Course\\Week - 5\\EthioMart_E-Commerce_NER\\notebooks\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "print(current_dir)\n",
    "\n",
    "# Get the parent directory\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "\n",
    "scripts_path = os.path.join(parent_dir, 'scripts')\n",
    "\n",
    "# Insert the path to the parent directory\n",
    "sys.path.insert(0, parent_dir)\n",
    "\n",
    "# Insert the path to the Scripts directory\n",
    "sys.path.insert(0, scripts_path)\n",
    "\n",
    "# Add the parent directory to the Python path\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the CoNLL dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.util import read_conll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>lables</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[ቴሌግራምtmemodernshoppingcenter, በአዲስ, ነገረ, ሁሌም,...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, B-Product, O, O, O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[ቴሌግራምtmemodernshoppingcenter, በአዲስ, ነገረ, ሁሌም,...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[ቴሌግራምtmemodernshoppingcenter, በአዲስ, ነገረ, ሁሌም,...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[ቴሌግራምtmemodernshoppingcenter, በአዲስ, ነገረ, ሁሌም,...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, B-Product, O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[ቴሌግራምtmemodernshoppingcenter, በአዲስ, ነገረ, ሁሌም,...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tokens  \\\n",
       "0  [ቴሌግራምtmemodernshoppingcenter, በአዲስ, ነገረ, ሁሌም,...   \n",
       "1  [ቴሌግራምtmemodernshoppingcenter, በአዲስ, ነገረ, ሁሌም,...   \n",
       "2  [ቴሌግራምtmemodernshoppingcenter, በአዲስ, ነገረ, ሁሌም,...   \n",
       "3  [ቴሌግራምtmemodernshoppingcenter, በአዲስ, ነገረ, ሁሌም,...   \n",
       "4  [ቴሌግራምtmemodernshoppingcenter, በአዲስ, ነገረ, ሁሌም,...   \n",
       "\n",
       "                                              lables  \n",
       "0  [O, O, O, O, O, O, O, O, O, B-Product, O, O, O...  \n",
       "1  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "2  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "3  [O, O, O, O, O, O, O, O, O, O, O, B-Product, O...  \n",
       "4  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = \"../data/conll.txt\"\n",
    "\n",
    "data = read_conll(PATH)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prep the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Encode the NER labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the unique labels\n",
    "unique_labels = ['I-PRICE', 'B-PRICE', \"B-LOCATION\", \"B-Product\", \"O\"]\n",
    "\n",
    "# now encode them to integers and create mappins between str and int and viceversa\n",
    "enc_to_str = {i: value for i,value in enumerate(unique_labels)}\n",
    "str_to_enc = {value: i for i,value in enumerate(unique_labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['lables'] = data['lables'].apply(lambda x: [str_to_enc[label] for label in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Convert the data frame into a huggingface dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\KifiyaAIM-Course\\Week - 5\\EthioMart_E-Commerce_NER\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, Features, Sequence, Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the features/internal structure of the dataset \n",
    "feats = Features({\n",
    "    'tokens': Sequence(Value('string')),\n",
    "    'lables': Sequence(Value('int32'))\n",
    "})\n",
    "\n",
    "# now convert the dataframe into a huggingface dataset  \n",
    "dataset = Dataset.from_pandas(data[['tokens', 'lables']], features=feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tokenize and align the lables for each of the models\n",
    "\n",
    "    The models are **mBert**, **bert-tiny-amharic**, **DistilBert**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.tokenizer import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load the tokenizers using custom class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tiny_tokenizer = Tokenizer(model_name='rasyosef/bert-tiny-amharic')\n",
    "bert_tiny_tokenizer.load_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\KifiyaAIM-Course\\Week - 5\\EthioMart_E-Commerce_NER\\venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "mbert_tokenizer = Tokenizer(model_name='bert-base-multilingual-cased')\n",
    "mbert_tokenizer.load_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "distil_bert_tokenizer = Tokenizer(model_name='distilbert-base-multilingual-cased')\n",
    "distil_bert_tokenizer.load_tokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tokenize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2161/2161 [00:00<00:00, 2324.75 examples/s]\n",
      "Map: 100%|██████████| 2161/2161 [00:00<00:00, 2545.01 examples/s]\n",
      "Map: 100%|██████████| 2161/2161 [00:00<00:00, 2612.06 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_bert_tiny = dataset.map(bert_tiny_tokenizer.tokenize_and_align, batched=True)\n",
    "tokenized_mbert = dataset.map(mbert_tokenizer.tokenize_and_align, batched=True)\n",
    "tokenized_distil_bert = dataset.map(distil_bert_tokenizer.tokenize_and_align, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Split the datasets into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_bert_tiny = tokenized_bert_tiny.train_test_split(test_size=0.1)\n",
    "train_test_mbert = tokenized_mbert.train_test_split(test_size=0.1)\n",
    "train_test_distill_bert = tokenized_distil_bert.train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Set training arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./training_result',\n",
    "    eval_strategy=\"epoch\",     # Evaluates at the end of each epoch\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,  # Batch size for training\n",
    "    per_device_eval_batch_size=16,   # Batch size for evaluation\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,               # Strength of weight decay\n",
    "    max_grad_norm=1.0,  # Gradient clipping\n",
    "    logging_dir='./logs',            # Directory for storing logs\n",
    "    logging_strategy=\"steps\",        # Log at regular intervals\n",
    "    logging_steps=50,                # Log every 50 steps\n",
    "    save_strategy=\"epoch\",           # Save model at the end of each epoch\n",
    "    report_to=\"none\",                # Only show logs in the output (no TensorBoard)\n",
    "    use_cpu=True,  # Force training to happen on CPU,\n",
    "    load_best_model_at_end=True,     # Load the best model (based on metric) at the end\n",
    "    metric_for_best_model=\"eval_loss\",# Metric used to determine the best model\n",
    "    save_total_limit=1,              # Only keep the best model, delete the others  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load and finetune the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\KifiyaAIM-Course\\Week - 5\\EthioMart_E-Commerce_NER\\venv\\lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\VICTUS 16\\.cache\\huggingface\\hub\\models--rasyosef--bert-tiny-amharic. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at rasyosef/bert-tiny-amharic and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "d:\\KifiyaAIM-Course\\Week - 5\\EthioMart_E-Commerce_NER\\venv\\lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\VICTUS 16\\.cache\\huggingface\\hub\\models--bert-base-multilingual-cased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "d:\\KifiyaAIM-Course\\Week - 5\\EthioMart_E-Commerce_NER\\venv\\lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\VICTUS 16\\.cache\\huggingface\\hub\\models--distilbert-base-multilingual-cased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "bert_tiny_model = AutoModelForTokenClassification.from_pretrained(\"rasyosef/bert-tiny-amharic\", num_labels=len(unique_labels))\n",
    "mbert_model = AutoModelForTokenClassification.from_pretrained(\"bert-base-multilingual-cased\", num_labels=len(unique_labels))\n",
    "distil_bert_model = AutoModelForTokenClassification.from_pretrained(\"distilbert-base-multilingual-cased\", num_labels=len(unique_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Define an evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "def compute_metrics(p):\n",
    "    \"\"\"\n",
    "    A custom metrics function that calculates matrix\n",
    "    \"\"\"\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_labels = [[label for label in label_row if label != -100] for label_row in labels]\n",
    "    predicted_labels = [[pred for pred, true in zip(pred_row, true_row) if true != -100] for pred_row, true_row in zip(predictions, labels)]\n",
    "\n",
    "    # Flatten the lists\n",
    "    true_labels_flat = [item for sublist in true_labels for item in sublist]\n",
    "    predicted_labels_flat = [item for sublist in predicted_labels for item in sublist]\n",
    "\n",
    "    # Calculate metrics\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(true_labels_flat, predicted_labels_flat, average='weighted', zero_division=True)\n",
    "\n",
    "    return {\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Define trainers for models"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns\n",
    "train_test_bert_tiny = train_test_bert_tiny.rename_columns({\"lables\" : \"labels\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_distill_bert = train_test_distill_bert.rename_columns({\"lables\" : \"labels\"})\n",
    "train_test_mbert = train_test_mbert.rename_columns({\"lables\" : \"labels\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
=======
   "execution_count": 20,
>>>>>>> fdc6f69e2f3f573b680ac7e488d0705c5f49d5f9
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "bert_tiny_trainer = Trainer(\n",
    "    model=bert_tiny_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_test_bert_tiny['train'],\n",
    "    eval_dataset=train_test_bert_tiny['test'],\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "mbert_model_trainer = Trainer(\n",
    "    model=mbert_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_test_bert_tiny['train'],\n",
    "    eval_dataset=train_test_bert_tiny['test'],\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "distil_bert_trainer = Trainer(\n",
    "    model=distil_bert_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_test_bert_tiny['train'],\n",
    "    eval_dataset=train_test_bert_tiny['test'],\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
<<<<<<< HEAD
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Train and evaluate the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/366 [22:50<?, ?it/s]\n",
      "                                                \n",
      " 14%|█▍        | 52/366 [00:03<00:18, 17.33it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.278, 'grad_norm': 0.7554197907447815, 'learning_rate': 1.726775956284153e-05, 'epoch': 0.41}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 28%|██▊       | 102/366 [00:06<00:15, 16.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2671, 'grad_norm': 0.6453912258148193, 'learning_rate': 1.4535519125683062e-05, 'epoch': 0.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 122/366 [00:07<00:14, 17.28it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "                                                 \n",
      "\u001b[A                                              \n",
      " 33%|███▎      | 122/366 [00:07<00:14, 17.28it/s]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2434251457452774, 'eval_precision': 0.952822911276246, 'eval_recall': 0.9503586473869976, 'eval_f1': 0.9261697174243163, 'eval_runtime': 0.2309, 'eval_samples_per_second': 939.679, 'eval_steps_per_second': 60.624, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 42%|████▏     | 152/366 [00:09<00:13, 16.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.251, 'grad_norm': 0.5626601576805115, 'learning_rate': 1.1803278688524591e-05, 'epoch': 1.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 55%|█████▌    | 202/366 [00:12<00:09, 16.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2277, 'grad_norm': 0.7277026772499084, 'learning_rate': 9.071038251366122e-06, 'epoch': 1.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 242/366 [00:15<00:07, 16.81it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "                                                 \n",
      "\u001b[A                                              \n",
      " 67%|██████▋   | 244/366 [00:15<00:07, 16.81it/s]\n",
      " 67%|██████▋   | 245/366 [00:15<00:11, 10.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.20211569964885712, 'eval_precision': 0.9590353259867578, 'eval_recall': 0.957190026186952, 'eval_f1': 0.9387654496617406, 'eval_runtime': 0.2363, 'eval_samples_per_second': 918.22, 'eval_steps_per_second': 59.24, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 69%|██████▊   | 251/366 [00:16<00:08, 13.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2114, 'grad_norm': 0.701259195804596, 'learning_rate': 6.338797814207651e-06, 'epoch': 2.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 82%|████████▏ | 301/366 [00:19<00:04, 15.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2037, 'grad_norm': 0.6918941140174866, 'learning_rate': 3.6065573770491806e-06, 'epoch': 2.46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 96%|█████████▌| 351/366 [00:22<00:00, 15.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1987, 'grad_norm': 0.7283041477203369, 'learning_rate': 8.743169398907105e-07, 'epoch': 2.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 365/366 [00:23<00:00, 14.97it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "                                                 \n",
      "\u001b[A                                              \n",
      "100%|██████████| 366/366 [00:23<00:00, 14.97it/s]\n",
      "                                                 \n",
      "100%|██████████| 366/366 [00:23<00:00, 15.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1816592663526535, 'eval_precision': 0.9598701110455059, 'eval_recall': 0.9581008766936127, 'eval_f1': 0.9408770928314705, 'eval_runtime': 0.262, 'eval_samples_per_second': 828.282, 'eval_steps_per_second': 53.438, 'epoch': 3.0}\n",
      "{'train_runtime': 23.7793, 'train_samples_per_second': 245.255, 'train_steps_per_second': 15.392, 'train_loss': 0.23209489759851676, 'epoch': 3.0}\n",
      "########## Started training for bert_tiny_amharic ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 14%|█▎        | 50/366 [02:04<12:54,  2.45s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2311, 'grad_norm': 1.16489577293396, 'learning_rate': 1.726775956284153e-05, 'epoch': 0.41}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 27%|██▋       | 100/366 [04:04<09:51,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0327, 'grad_norm': 0.5814956426620483, 'learning_rate': 1.4535519125683062e-05, 'epoch': 0.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 122/366 [04:52<07:55,  1.95s/it]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "                                                 \n",
      "\u001b[A                                              \n",
      " 33%|███▎      | 122/366 [04:59<07:55,  1.95s/it]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.01548036653548479, 'eval_precision': 0.9961335793924762, 'eval_recall': 0.9957873164066947, 'eval_f1': 0.9958147640169689, 'eval_runtime': 7.0033, 'eval_samples_per_second': 30.986, 'eval_steps_per_second': 1.999, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 41%|████      | 150/366 [06:02<07:52,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0149, 'grad_norm': 0.21399249136447906, 'learning_rate': 1.1803278688524591e-05, 'epoch': 1.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 55%|█████▍    | 200/366 [07:52<06:03,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0101, 'grad_norm': 0.3484780192375183, 'learning_rate': 9.071038251366122e-06, 'epoch': 1.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 244/366 [09:28<03:58,  1.96s/it]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "                                                 \n",
      "\u001b[A                                              \n",
      " 67%|██████▋   | 244/366 [09:35<03:58,  1.96s/it]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.004352310206741095, 'eval_precision': 0.999324181454433, 'eval_recall': 0.9993168621200046, 'eval_f1': 0.9993138322150186, 'eval_runtime': 6.9999, 'eval_samples_per_second': 31.001, 'eval_steps_per_second': 2.0, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 68%|██████▊   | 250/366 [09:49<05:01,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0089, 'grad_norm': 0.34992271661758423, 'learning_rate': 6.338797814207651e-06, 'epoch': 2.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 82%|████████▏ | 300/366 [11:39<02:24,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0066, 'grad_norm': 0.08240452408790588, 'learning_rate': 3.6065573770491806e-06, 'epoch': 2.46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 96%|█████████▌| 350/366 [13:29<00:35,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0051, 'grad_norm': 0.21176043152809143, 'learning_rate': 8.743169398907105e-07, 'epoch': 2.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 366/366 [14:04<00:00,  1.95s/it]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "                                                 \n",
      "\u001b[A                                              \n",
      "100%|██████████| 366/366 [14:12<00:00,  1.95s/it]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.002943056635558605, 'eval_precision': 0.999551866793785, 'eval_recall': 0.9995445747466697, 'eval_f1': 0.9995458650750785, 'eval_runtime': 7.0211, 'eval_samples_per_second': 30.907, 'eval_steps_per_second': 1.994, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      "100%|██████████| 366/366 [14:14<00:00,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 854.5604, 'train_samples_per_second': 6.825, 'train_steps_per_second': 0.428, 'train_loss': 0.042521417364885246, 'epoch': 3.0}\n",
      "########## Started training for distill_bert ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "bert_tiny_trainer.train()\n",
    "print(\"########## Started training for bert_tiny_amharic ##########\")\n",
    "\n",
    "distil_bert_trainer.train()\n",
    "print(\"########## Started training for distill_bert ##########\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
=======
>>>>>>> fdc6f69e2f3f573b680ac7e488d0705c5f49d5f9
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
